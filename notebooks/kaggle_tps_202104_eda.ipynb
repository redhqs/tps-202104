{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "serious-dallas",
   "metadata": {},
   "source": [
    "### EDA of Apr 2021 kaggle TPS\n",
    "\n",
    "This notebook is used to do initial exploration of the training data, get an understanding of the distribution and completeness of each column and relationship with the target. The aim is to build enough of an understanding to make some baseline predictions.\n",
    "\n",
    "[Data dictionary](https://www.kaggle.com/c/tabular-playground-series-apr-2021/data?select=train.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raised-specialist",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for EDA\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "personal-algorithm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for inference\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "narrow-davis",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/raw/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floating-bacteria",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metric-range",
   "metadata": {},
   "source": [
    "We can get some idea of what to expect from the first few rows - next to look at the distribution of each more closely, and think about how to handle missing data and how to encode sex, class, port of embarkation etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opposed-hanging",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organic-applicant",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabulous-raising",
   "metadata": {},
   "source": [
    "So with a 43% survival rate, hopefully we can do better than predicting everyone as lost at 57%. Noticing a lot of missing ages, not so many missing fares. Assuming the passenger IDs are all ok, on to survival and lots of plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "checked-martin",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(data=df, x=\"Survived\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bottom-break",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(data=df, x=\"Pclass\", hue=\"Survived\", multiple=\"stack\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "german-vinyl",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['Sex', 'Survived'])['Survived'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monthly-cigarette",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Sex'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "received-dealing",
   "metadata": {},
   "source": [
    "No missing values here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accepted-heater",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(data=df, x=\"Age\", hue=\"Survived\", kind=\"kde\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "special-region",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Age'].isnull().sum()\n",
    "#3.3% of values for Age are missing - may be able to impute from other cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outer-workshop",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(data=df, x=\"SibSp\", hue=\"Survived\", multiple=\"stack\")\n",
    "# SibSp and Parch will be returned to after a baseline established"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bibliographic-region",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(data=df, x=\"Parch\", hue=\"Survived\", multiple=\"stack\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adverse-effect",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['Ticket'].nunique())\n",
    "print(df['Ticket'].isnull().sum())\n",
    "# After baseline established, should be possible to link families using ticket, cabin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nutritional-challenge",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['Fare'].isnull().sum())\n",
    "sns.displot(data=df, x=\"Fare\", hue=\"Survived\", kind=\"kde\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driven-switch",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['Cabin'].nunique())\n",
    "print(df['Cabin'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southwest-northeast",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['Embarked', 'Survived'])['Survived'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ranking-greeting",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Embarked'].isnull().sum()\n",
    "# A few missing here, to investigate after the first iteration is complete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abandoned-batman",
   "metadata": {},
   "source": [
    "There is work to be done to work out the best way to impute missing data around Age, SibSp, Parch, Ticket, Cabin, and Embarked, but first I want to compare a quick mvp to a dummy benchmark to set a baseline. For this the categorical columns will need to be encoded, nulls filled with temporary junk, after breaking out to a train/test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empty-flooring",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df[['Survived']]\n",
    "features = df[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "south-kingdom",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "maritime-drove",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical = ['Pclass', 'Sex', 'Embarked']\n",
    "numeric = ['Age', 'SibSp', 'Parch', 'Fare']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustainable-greek",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[numeric].describe()\n",
    "# df['value'] = df['value'].fillna(df.groupby('category')['value'].transform('mean'))\n",
    "# df['value'] = df['value'].fillna(df['value'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ruled-mortgage",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['Age'] = X_train['Age'].fillna(X_train['Age'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noticed-population",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['Fare'] = X_train['Fare'].fillna(X_train['Fare'].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wooden-cameroon",
   "metadata": {},
   "source": [
    "With numerical blanks filled with median values (remember, this is an mvp) and a train test split specified, numeric columns are scaled and categorical columns are one hot encoded. Initial estimator is logistic regression for a simple binary classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relevant-aerospace",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = make_column_transformer(\n",
    "    (StandardScaler(), numeric),\n",
    "    (OneHotEncoder(drop='if_binary'), categorical), \n",
    "    remainder='passthrough')\n",
    "\n",
    "model = make_pipeline(\n",
    "    preprocessor,\n",
    "    LogisticRegression())\n",
    "\n",
    "_ = model.fit(X_train, np.ravel(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floral-catering",
   "metadata": {},
   "source": [
    "To set a floor with a baseline, let's see the classification report if we predict the ship going down with no survivors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collect-savings",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_dummy = np.full(shape=len(y_pred), fill_value=0)\n",
    "print(classification_report(y_train, y_dummy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innovative-munich",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_train)\n",
    "print(classification_report(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accredited-progressive",
   "metadata": {},
   "source": [
    "Performance on training data is ok, better than guessing that no one makes it at least."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explicit-chance",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_test['Age'] = X_test['Age'].fillna(X_train['Age'].median())\n",
    "X_test['Fare'] = X_test['Fare'].fillna(X_train['Fare'].median())\n",
    "y_unseen = model.predict(X_test)\n",
    "print(classification_report(y_test, y_unseen))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "social-eight",
   "metadata": {},
   "source": [
    "To finish with the same steps are carried out on the test set and submitted. With more time I'd want to do EDA on the test set to look for drift in each category, but for right now I'm interested in how this model performs without any additional intervention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "refined-machine",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to code and run if running inference on test set for kaggle submission\n",
    "file_out = \"../data/inference/basic_eda_gb.csv\"\n",
    "test_df = pd.read_csv(\"../data/raw/test.csv\")\n",
    "test_df.head()\n",
    "test_features = test_df[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']]\n",
    "test_features['Age'] = test_features['Age'].fillna(X_train['Age'].median())\n",
    "test_features['Fare'] = test_features['Fare'].fillna(X_train['Fare'].median())\n",
    "test_unseen = model_gb.predict(test_features)\n",
    "test_df['Survived'] = test_unseen.tolist()\n",
    "test_submission = test_df[['PassengerId', 'Survived']]\n",
    "test_submission.to_csv(file_out, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "combined-sleep",
   "metadata": {},
   "source": [
    "This initial submission achieved a leaderboard score of 0.77000. \n",
    "\n",
    "Next steps are:\n",
    "- using an ensemble algorithm on the same data as logistic regression above, measure uplift\n",
    "- look at more intelligent ways of filling nulls, making better use of tickets and cabins to see if that affects survival."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustained-opinion",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "# yes there needs to be a proper CV grid search eventually - submitting this as is to kaggle yielded leaderboard score of 0.78642\n",
    "model_gb = make_pipeline(\n",
    "    preprocessor,\n",
    "    GradientBoostingClassifier())\n",
    "\n",
    "_ = model_gb.fit(X_train, np.ravel(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behind-attitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_gb = model_gb.predict(X_train)\n",
    "print(classification_report(y_train, y_pred_gb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equivalent-hanging",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_unseen_gb = model_gb.predict(X_test)\n",
    "print(classification_report(y_test, y_unseen_gb))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
