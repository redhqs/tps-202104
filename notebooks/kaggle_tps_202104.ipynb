{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "serious-dallas",
   "metadata": {},
   "source": [
    "## Apr 2021 kaggle TPS\n",
    "\n",
    "This notebook carries out initial exploration of the training data to get an understanding of the distribution and completeness of each column and relationship to the target. \n",
    "\n",
    "The aim is to build a broad understanding, enough to make some small feature engineering decisions - and make some baseline predictions.\n",
    "\n",
    "Synthanic [Data Dictionary](https://www.kaggle.com/c/tabular-playground-series-apr-2021/data?select=train.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raised-specialist",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for EDA\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.graphics.mosaicplot import mosaic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "personal-algorithm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for inference and scoring\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import make_column_transformer, ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, log_loss, make_scorer\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d6da02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa19d108",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"paper\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b79103",
   "metadata": {},
   "source": [
    "### EDA\n",
    "We can get some idea of what to expect from the first few rows - next to look at the distribution of each more closely, and think about how to handle missing data and how to encode sex, class, port of embarkation etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floating-bacteria",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/raw/train.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opposed-hanging",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1180ed4",
   "metadata": {},
   "source": [
    "Note null values in `Age`, `Ticket`, `Fare`, `Cabin` and `Embarked`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organic-applicant",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabulous-raising",
   "metadata": {},
   "source": [
    "So with a 43% survival rate, hopefully we can do better than predicting everyone as lost at 57%. Noticing a lot of missing ages, not so many missing fares. \n",
    "\n",
    "For the purposes of this challend I'm assuming the passenger IDs are all ok - so on to survival, and lots of plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "checked-martin",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(data=df, x=\"Survived\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bottom-break",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(data=df, x=\"Pclass\", hue=\"Survived\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e6df58",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(data=df, x=\"Sex\", hue=\"Survived\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "received-dealing",
   "metadata": {},
   "source": [
    "No missing values here - but note significant differences in outcomes between male and female."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd2b638",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['Sex', 'Survived'])['Survived'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30621c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mosaic(df, ['Pclass', 'Sex', 'Survived'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accepted-heater",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(data=df, x=\"Age\", hue=\"Survived\", kind=\"kde\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c708f6bf",
   "metadata": {},
   "source": [
    "Worth noting that the other seaborn displots `hist` and `ecdf` can show up some interesting quirks in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outer-workshop",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.countplot(data=df, x=\"SibSp\", hue=\"Survived\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498cfa1a",
   "metadata": {},
   "source": [
    "The SibSp chart is dominated by the number of passengers with 0 or 1 - let's zoom in on 2+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc9ea78",
   "metadata": {},
   "outputs": [],
   "source": [
    "dplot = sns.countplot(data=df, x=\"SibSp\", hue=\"Survived\")\n",
    "dplot.set(xlim=(1.5, 6.5))\n",
    "dplot.set(ylim=(0.0, 2100.0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bibliographic-region",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.countplot(data=df, x=\"Parch\", hue=\"Survived\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c277846",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dplot = sns.countplot(data=df, x=\"Parch\", hue=\"Survived\")\n",
    "dplot.set(xlim=(0.5, 7.5))\n",
    "dplot.set(ylim=(0, 8000))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea97f3bd",
   "metadata": {},
   "source": [
    "Values (and the lack of) from 5+ in `SibSp` and 4+ in `Parch` may lead to overfitting in the unseen data - binning these into broader categories could be more robust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adverse-effect",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of unique tickets: {}\".format(df['Ticket'].nunique()))\n",
    "print(\"Number of null tickets: {}\".format(df['Ticket'].isnull().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nutritional-challenge",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.displot(data=df, x=\"Fare\", hue=\"Survived\", kind=\"kde\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secure-pledge",
   "metadata": {},
   "source": [
    "Those paying < 100 did not do so well. Let's take a closer look at passengers who paid more:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ignored-smart",
   "metadata": {},
   "outputs": [],
   "source": [
    "dplot = sns.displot(data=df, x=\"Fare\", hue=\"Survived\", kind=\"kde\")\n",
    "dplot.set(xlim=(100, 800))\n",
    "dplot.set(ylim=(0, 0.001))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southwest-northeast",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of passengers missing embarkation port: {}\".format(df['Embarked'].isnull().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a498e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_fill = {'Embarked': 'X'}\n",
    "df.fillna(value=emb_fill, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ranking-greeting",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.groupby(['Embarked', 'Survived'])['Survived'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rapid-ancient",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(data=df, x=\"Embarked\", hue=\"Survived\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29894c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Cabin'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da869df",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of unique cabin IDs: {}\".format(df['Cabin'].nunique()))\n",
    "print(\"Number of null cabin IDs: {}\".format(df['Cabin'].isnull().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cf8e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['CabinArea'] = df['Cabin'].fillna('X').map(lambda x: x[0].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990429e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.countplot(data=df, x=\"CabinArea\", hue=\"Survived\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be80342",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Ticket'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f00516",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Number of unique ticket IDs: {}\".format(df['Ticket'].nunique()))\n",
    "print(\"Number of null ticket IDs: {}\".format(df['Ticket'].isnull().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663b3820",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TicketStub'] = df['Ticket'].fillna('X').map(lambda x:str(x).split()[0] if len(str(x).split()) > 1 else 'X') \\\n",
    "                               .str.upper() \\\n",
    "                               .replace(\"[^a-zA-Z0-9]*\", \"\", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac63b033",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TicketStub'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b40285",
   "metadata": {},
   "source": [
    "Prior to cleaning, lots of similar tickets - for example 'SC/PARIS', 'S.C./PARIS', 'SC/Paris'. Decided to make all tickets uppercase and remove any punctuation.\n",
    "\n",
    "Unsure if overkill but groups similar-ish tickets together and will process any tickets in the test set in exactly the same way. Some tickets are much more unlucky than others:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e01b67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.groupby(['TicketStub'], sort=False, dropna=True)['Survived'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f72b33a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.groupby(['TicketStub'], sort=False, dropna=True)['Survived'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abandoned-batman",
   "metadata": {},
   "source": [
    "### Modelling\n",
    "\n",
    "After initial EDA, the aim is to put together some straightforward models:\n",
    "- Baseline, assuming no survivors\n",
    "- Logistic regression classifier\n",
    "- Gradient boosting classifier\n",
    "\n",
    "There is also work to be done to work out the best way to impute missing data around `Age`, `SibSp`, `Parch`, `Ticket`, `Cabin`, and `Embarked`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empty-flooring",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df[['Survived']]\n",
    "features = df[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked', 'CabinArea', 'TicketStub']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "south-kingdom",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f81639",
   "metadata": {},
   "source": [
    "At this point I don't want to make any assumptions about the data, apart from separating categorical and numeric columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "maritime-drove",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical = ['Pclass', 'Sex', 'Embarked', 'CabinArea', 'TicketStub']\n",
    "numeric = ['Age', 'SibSp', 'Parch', 'Fare']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustainable-greek",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[numeric].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95fd277",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(data=X_train, x='Fare', kind='kde') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1cff06",
   "metadata": {},
   "source": [
    "The idea here is to replace missing values of `Fare` with the median of `Pclass`, and then retain that value for the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stable-dietary",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fare_map = X_train[['Fare', 'Pclass']].dropna().groupby('Pclass').median().to_dict()\n",
    "X_train['Fare'] = X_train['Fare'].fillna(X_train['Pclass'].map(fare_map['Fare']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f9ca45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fare_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0024b566",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(data=X_train, x='Age', kind='kde')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39c2c85",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "And to do something similar with `Age`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4269dc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "age_map = X_train[['Age', 'Sex']].dropna().groupby(['Sex']).median().to_dict()\n",
    "X_train['Age'] = X_train['Age'].fillna(X_train['Sex'].map(age_map['Age']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44993a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "age_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc66a712",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(data=X_train, x='Age', kind='kde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572deff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wooden-cameroon",
   "metadata": {},
   "source": [
    "With nulls filled and a train/test split specified, numeric columns are scaled with a `StandardScaler` and categorical columns are transformed with a `OneHotEncoder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5a0756",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical = ['Pclass', 'Sex', 'Embarked', 'CabinArea', 'TicketStub']\n",
    "numeric = ['Age', 'SibSp', 'Parch', 'Fare']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relevant-aerospace",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = make_column_transformer(\n",
    "    (StandardScaler(), numeric),\n",
    "    (OneHotEncoder(drop='if_binary'), categorical), \n",
    "    remainder='drop')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ba7255",
   "metadata": {},
   "source": [
    "Initial estimator is `LogisticRegression` as a baseline for a binary classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dc1a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_pipeline(\n",
    "    preprocessor,\n",
    "    LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5))\n",
    "\n",
    "_ = model.fit(X_train, np.ravel(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floral-catering",
   "metadata": {},
   "source": [
    "To set a floor for model performance, let's see a classification report as if the ship is predicted as going down with no survivors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collect-savings",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_train)\n",
    "y_dummy = np.full(shape=len(y_pred), fill_value=0)\n",
    "print(classification_report(y_train, y_dummy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innovative-munich",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accredited-progressive",
   "metadata": {},
   "source": [
    "Performance on training data is ok at 77% overall accuracy; better than guessing that no one makes it at least, at 57% accuracy. Compare to the held out test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95fca6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explicit-chance",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# using age and fare maps from X_train, no peeking\n",
    "X_test['Fare'] = X_test['Fare'].fillna(X_test['Pclass'].map(fare_map['Fare']))\n",
    "X_test['Age'] = X_test['Age'].fillna(X_test['Sex'].map(age_map['Age']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c2c081",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_unseen = model.predict(X_test)\n",
    "print(classification_report(y_test, y_unseen))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "social-eight",
   "metadata": {},
   "source": [
    "To finish with the same steps are carried out on the competition test set and submitted. With more time you could do EDA on the test set to look for drift across each category, but for right now I'm interested in how this model performs without any additional intervention."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "combined-sleep",
   "metadata": {},
   "source": [
    "This initial submission achieved a leaderboard score of 0.77000. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustained-opinion",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gb = make_pipeline(\n",
    "    preprocessor,\n",
    "    XGBClassifier(objective=\"binary:logistic\", random_state=42))\n",
    "\n",
    "_ = model_gb.fit(X_train, np.ravel(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behind-attitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_gb = model_gb.predict(X_train)\n",
    "print(classification_report(y_train, y_pred_gb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equivalent-hanging",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_unseen_gb = model_gb.predict(X_test)\n",
    "print(classification_report(y_test, y_unseen_gb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b78641",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sticky-cleaning",
   "metadata": {},
   "source": [
    "The ensemble method achieved a leaderboard score of 0.78642 out of the box. Let's see what a moderate parameter search yields. \n",
    "\n",
    "Using `mini_param` to test the grid search works as expected before kicking off thousands of runs with different classifiers"
   ]
  },
  {
   "cell_type": "raw",
   "id": "843af1a4",
   "metadata": {},
   "source": [
    "# scikit-learn GradientBoostingClassifier parameters\n",
    "\n",
    "mini_param = {\n",
    "    \"loss\":[\"deviance\"], \n",
    "    \"learning_rate\": [0.2],\n",
    "    \"n_estimators\": [250],\n",
    "    \"subsample\": [0.8],\n",
    "    \"criterion\": [\"mse\"],\n",
    "    \"max_depth\": [15],\n",
    "    \"max_features\": [\"sqrt\"],\n",
    "    \"random_state\": [42],\n",
    "    \"validation_fraction\": [0.1],\n",
    "    \"n_iter_no_change\": [10]}\n",
    "\n",
    "param_grid = {\n",
    "    \"loss\":[\"deviance\", \"exponential\"], \n",
    "    \"learning_rate\": [0.01, 0.025, 0.05, 0.075, 0.1, 0.15, 0.2],\n",
    "    \"n_estimators\":[100, 250, 500],\n",
    "    \"subsample\":[0.5, 0.8, 0.9, 1.0],\n",
    "    \"criterion\": [\"friedman_mse\",  \"mse\"],\n",
    "    \"max_depth\":[6, 9, 15],\n",
    "    \"max_features\":[\"log2\",\"sqrt\"],\n",
    "    \"random_state\":[42],\n",
    "    \"validation_fraction\":[0.1],\n",
    "    \"n_iter_no_change\":[5, 10]}"
   ]
  },
  {
   "cell_type": "raw",
   "id": "57151eac",
   "metadata": {},
   "source": [
    "# scikit-learn SGDClassifier parameters\n",
    "\n",
    "mini_param = {\n",
    "    'loss':['hinge'],\n",
    "    'penalty':['l2'],\n",
    "    'alpha':[0.0001],\n",
    "    'l1_ratio':[0.15],\n",
    "    'fit_intercept':[True],\n",
    "    'max_iter':[1000],\n",
    "    'tol':[0.001],\n",
    "    'shuffle':[True],\n",
    "    'verbose':[0],\n",
    "    'epsilon':[0.1],\n",
    "    'n_jobs':[-1],\n",
    "    'random_state':[42],\n",
    "    'learning_rate':['optimal'],\n",
    "    'eta0':[0.0],\n",
    "    'power_t':[0.5],\n",
    "    'early_stopping':[False],\n",
    "    'validation_fraction':[0.1],\n",
    "    'n_iter_no_change':[5],\n",
    "    'class_weight':[None],\n",
    "    'warm_start':[False],\n",
    "    'average':[False]}\n",
    "\n",
    "param_grid = {\n",
    "    'loss':['hinge', 'modified_huber'],\n",
    "    'penalty':['l2', 'l1'],\n",
    "    'alpha':[0.0001, 0.001, 0.00001, 0.05],\n",
    "    'fit_intercept':[True, False],\n",
    "    'max_iter':[1000],\n",
    "    'tol':[0.001],\n",
    "    'shuffle':[True],\n",
    "    'verbose':[1],\n",
    "    'n_jobs':[-1],\n",
    "    'random_state':[42],\n",
    "    'learning_rate':['optimal', 'adaptive'],\n",
    "    'eta0':[0.01, 0.001],\n",
    "    'early_stopping':[False, True],\n",
    "    'validation_fraction':[0.1],\n",
    "    'n_iter_no_change':[5, 10],\n",
    "    'class_weight':[None, 'balanced'],\n",
    "    'warm_start':[False, True]}"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e0ec8fed",
   "metadata": {},
   "source": [
    "# xgboost parameters\n",
    "\n",
    "no_param = {'random_state':[42],'base_score':[0]}\n",
    "\n",
    "xgb_param = {\n",
    "    'random_state':[42],'base_score':[0.3, 0.4], 'verbosity':[1],\n",
    "    'booster':['gbtree'], 'eval_metric':['logloss'],\n",
    "    'learning_rate':[0.1, 0.2, 0.3], 'max_depth':[3, 5, 9], 'subsample':[0.5, 0.8, 1], 'min_child_weight':[1, 2, 4],\n",
    "    'reg_lambda':[1], 'reg_alpha':[0, 0.1, 1], 'colsample_bytree':[0.5, 0.8, 1],\n",
    "    'importance_type':['gain'], 'n_estimators':[100, 500], 'num_parallel_tree':[1], \n",
    "    'validate_parameters':[True]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3becbc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = XGBClassifier() # SGDClassifier(), GradientBoostingClassifier()\n",
    "scores = {'acc': make_scorer(accuracy_score), \n",
    "          'f1': make_scorer(f1_score)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d719a43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_process = ColumnTransformer([\n",
    "    ('num', StandardScaler(), numeric),\n",
    "    ('cat', OneHotEncoder(), categorical)], n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ca9bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(clf, param_grid=xgb_param, refit=True, scoring='accuracy', cv=3, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7be496",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_prep = pre_process.fit_transform(X_train)\n",
    "grid.fit(X_prep, np.ravel(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a563a6f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963f8d3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_test_prep = pre_process.transform(X_test)\n",
    "y_test_acc = grid.predict(X_test_prep)\n",
    "print(classification_report(y_test, y_test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33691201",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_out = pd.DataFrame.from_dict(grid.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f06825",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_out_loc = \"../data/scores/grid_out_xgb.csv\"\n",
    "grid_out.to_csv(grid_out_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97d44e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", None)\n",
    "#grid_out.sort_values(['rank_test_f1','rank_test_acc'], ascending=[True, True]).head(5)\n",
    "grid_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86251ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_out.sort_values(['rank_test_acc','rank_test_f1'], ascending=[True, True]).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30dbfbc",
   "metadata": {},
   "source": [
    "Taking the parameters of the two `GradientBoostingClassifier` models with best f1 and accuracy for submission to kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6548f982",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_param = {'alpha': 0.001, 'class_weight': None, 'early_stopping': False, 'eta0': 0.01, 'fit_intercept': False, 'learning_rate': 'optimal', 'loss': 'modified_huber', 'max_iter': 1000, 'n_iter_no_change': 5, 'n_jobs': -1, 'penalty': 'l1', 'random_state': 42, 'shuffle': True, 'tol': 0.001, 'validation_fraction': 0.1, 'verbose': 1, 'warm_start': False}\n",
    "f1_param = {'alpha': 0.001, 'class_weight': 'balanced', 'early_stopping': True, 'eta0': 0.01, 'fit_intercept': True, 'learning_rate': 'optimal', 'loss': 'modified_huber', 'max_iter': 1000, 'n_iter_no_change': 10, 'n_jobs': -1, 'penalty': 'l2', 'random_state': 42, 'shuffle': True, 'tol': 0.001, 'validation_fraction': 0.1, 'verbose': 1, 'warm_start': False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4dd161",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf_acc = SGDClassifier(**acc_param).fit(X_prep, np.ravel(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cc381b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_acc = clf_acc.predict(X_prep)\n",
    "print(classification_report(y_train, y_pred_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54411d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_prep = pre_process.transform(X_test)\n",
    "y_test_acc = clf_acc.predict(X_test_prep)\n",
    "print(classification_report(y_test, y_test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691e8d1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf_f1 = SGDClassifier(**f1_param).fit(X_prep, np.ravel(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692a9dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_f1 = clf_f1.predict(X_prep)\n",
    "print(classification_report(y_train, y_pred_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1ceac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_f1 = clf_f1.predict(X_test_prep)\n",
    "print(classification_report(y_test, y_test_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00a0e0c",
   "metadata": {},
   "source": [
    "Both of these models seem consistent with each other - I don't think much of an improvement can be expected on the held out kaggle set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d9939e",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_out = \"../data/inference/cv_acc_xgb.csv\""
   ]
  },
  {
   "cell_type": "raw",
   "id": "70437826",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# convert to code and run if running inference on test set for kaggle submission\n",
    "test_df = pd.read_csv(\"../data/raw/test.csv\")\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb2ba44",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d76f96",
   "metadata": {},
   "source": [
    "Applying same transformations used on training set to test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee88d292",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "na_fill = {'Embarked': 'X', 'Cabin': 'X', 'Ticket': 'X'}\n",
    "test_df.fillna(value=na_fill, inplace=True)\n",
    "test_df['TicketStub'] = test_df['Ticket'].fillna('X').map(lambda x:str(x).split()[0] if len(str(x).split()) > 1 else 'X') \\\n",
    "                               .str.upper() \\\n",
    "                               .replace(\"[^a-zA-Z0-9]*\", \"\", regex=True)\n",
    "test_df['CabinArea'] = test_df['Cabin'].fillna('X').map(lambda x: x[0].strip())\n",
    "test_df['Fare'] = test_df['Fare'].fillna(test_df['Pclass'].map(fare_map['Fare']))\n",
    "test_df['Age'] = test_df['Age'].fillna(test_df['Sex'].map(age_map['Age']))\n",
    "test_features = test_df[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked', 'CabinArea', 'TicketStub']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945005f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features['TicketStub'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750a7e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prep = pre_process.transform(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87195351",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_xgb = grid.predict(test_prep)\n",
    "test_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce3b00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['Survived'] = test_xgb.tolist()\n",
    "test_submission = test_df[['PassengerId', 'Survived']]\n",
    "test_submission.to_csv(file_out, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac9ab70",
   "metadata": {},
   "source": [
    "Although they differ in 1638 cases, both f1 and accuracy GBM models score 0.80394 on the leaderboard - perhaps they differ on the 75% held back for evaluation after the challenge ends.\n",
    "\n",
    "In any case, cracking 80% accuracy on the public leaderboard caused a jump of hundreds of places, so it was good to see some positive impact from a good old fashioned grid search with a gradient boosting classifier.\n",
    "\n",
    "Next step should be to look into feature importance identify the cases that are consistently predicted incorrectly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
